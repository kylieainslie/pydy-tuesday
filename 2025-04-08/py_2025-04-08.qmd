---
title: "Pydy Tuesday 2025-04-08"
format: html
jupyter: python3
---

# Data Source
<img src="visits.png" style="float: right; width: 100px; height: auto; margin-right: 100px;" />

**Data Source:** This week we're exploring state-level results for medicare.gov "timely and effective care" measurements. As of 2025-04-06, the data is available at the [Centers for Medicare and Medicaid Services (CMS) website](https://data.cms.gov/provider-data/dataset/apyc-v239).

*Data provided by:* Tracy Teal (@tracykteal).

*Additional links:*
    - [a visualization by Kayla Zhu and Christina Kostandi at the Visual Capitalist](https://www.visualcapitalist.com/mapped-emergency-room-visit-times-by-state/) 

### Background
Emergency room wait times vary significantly across the United States depending on factors such as hospital resources, patient volume, and staffing levels, with some states facing delays that can stretch for more than three hours. In this short Quarto report, rather than focus on trying to visualise associations within the data or fit a model to explain an outcome variable of interest (e.g., wait times), I focus on the statistical basics that must be performed *before* a model can be fit to these data. Specifically, I determine the distribution that best fits time spent in the Emergency Department (ED).

## Set up
```{python}
# import required libraries
import os
import PyDyTuesday
import pandas as pd
from great_tables import GT
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import glm

# set working directory (only for line by line coding)
# comment out when rendering!
#os.getcwd()
#os.chdir(os.getcwd() + '/2025-04-08')
```

## Download data
We can download the data using the `PyDyTuesday` library and specifying the date. This will load several files:

- `care_state.csv`: data file.
- `visits.png`: image file.
- `meta.yaml`: meta data file with information about the data files and the curator.

The data dictionary is located [here](https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-04-08/readme.md#data-dictionary)

```{python}
# Download files from the week, which you can then read in locally
PyDyTuesday.get_date('2025-04-08')

df = pd.read_csv("care_state.csv", encoding='latin-1')
```

```{python}
(
    GT(df.head())
    .tab_header(title="Visit Times per State")
    .fmt_date(columns=["start_date", "end_date"])
    .fmt_number(columns=["score"])
)
```

## Data Exploration
Since there are numerous conditions reported in the data, let's restrict our analysis to those conditions for the Emergency Department (`condition = 'Emergency Department'`). We'll also restrict our analysis to measures of average time in the emergency department before being seen, which is `measure_id` code starting with `"OP_18b"`.
```{python}
df_ed = df[df['condition'] == 'Emergency Department']
df_ed_op = df_ed[df_ed['measure_id'].str.startswith('OP_18b')]
df_ed_op.head()
```

To get a feel for the data, let's plot the a histogram of the average time spent in the emergency department before being seen.
```{python}
plt.hist(df_ed_op['score'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Average time (in minutes) spent in ED')
plt.ylabel('Frequency')
plt.show()
```

From the histogram, we see that the time spent in the ED has a long tail, suggesting that a normal distribution (and thus, linear regression) would not be an approporiate assumption for the underlying distribution. The shape of the data looks like it might be gamma distributed. Let's check this by fitting a gamma distribution to the data and looking at a Q-Q plot.

```{python}
data = df_ed_op['score'].dropna()

# Fit a gamma distribution
shape, loc, scale = stats.gamma.fit(data)

stats.probplot(data, dist="gamma", sparams=(shape, loc, scale), plot=plt)
plt.title('Q-Q Plot: Gamma Distribution')
plt.show()

# Perform the Kolmogorov-Smirnov (K-S) test
ks_stat, ks_p_value = stats.kstest(data, 'gamma', args=(shape, loc, scale))

# Print the result
print(f'K-S Statistic: {ks_stat}, p-value: {ks_p_value}')
```

Based on these diagnostics, the Gamma distribution is not a good fit for our data. Let's try a couple of other distributions to see if we get a better fit. We'll fit the following distributions to our data:

- Normal
- Log-normal
- Weibull
- Exponential

We'll test each distribution's fit with our data using the Kolmogorov-Smirnov (K-S) test as we did above. A small K-S statistic indicates that the data and the fitted distribution are similar, whereas, a large K-S statistic indicates that there is a large discrepancy between the data and the fitted distribution.

```{python}
# Fit different distributions to data
params_normal = stats.norm.fit(data)                 # Normal Distribution
params_lognormal = stats.lognorm.fit(data, floc=0)   # Log-Normal Distribution
params_weibull = stats.weibull_min.fit(data, floc=0) # Weibull Distribution 
params_exponential = stats.expon.fit(data, floc=0)   # Exponential Distribution

# Perform KS test for each distribution
ks_stat_normal, p_value_normal = stats.kstest(data, 'norm', args=params_normal)
ks_stat_lognormal, p_value_lognormal = stats.kstest(data, 'lognorm', args=params_lognormal)
ks_stat_weibull, p_value_weibull = stats.kstest(data, 'weibull_min', args=params_weibull)
ks_stat_exponential, p_value_exponential = stats.kstest(data, 'expon', args=params_exponential)

# Print KS statistics and p-values
print(f"Normal KS Statistic: {ks_stat_normal}, p-value: {p_value_normal}")
print(f"Log-Normal KS Statistic: {ks_stat_lognormal}, p-value: {p_value_lognormal}")
print(f"Weibull KS Statistic: {ks_stat_weibull}, p-value: {p_value_weibull}")
print(f"Exponential KS Statistic: {ks_stat_exponential}, p-value: {p_value_exponential}")
```

We can also overlay each of these distributions over our histogram to visually check how they fit the data.
```{python}
plt.clf()

plt.hist(data, bins=30, color='skyblue', edgecolor='black', density=True, alpha=0.6)
x = np.linspace(min(data), max(data), 1000)

# Normal Distribution
pdf_normal = stats.norm.pdf(x, *params_normal)
plt.plot(x, pdf_normal, 'r-', label="Normal Fit")

# Log-Normal Distribution
pdf_lognormal = stats.lognorm.pdf(x, *params_lognormal)
plt.plot(x, pdf_lognormal, 'g-', label="Log-Normal Fit")

# Weibull Distribution
pdf_weibull = stats.weibull_min.pdf(x, *params_weibull)
plt.plot(x, pdf_weibull, 'b-', label="Weibull Fit")

# Exponential Distribution
pdf_exponential = stats.expon.pdf(x, *params_exponential)
plt.plot(x, pdf_exponential, 'm-', label="Exponential Fit")

# Labels and legend
plt.xlabel('Average time (in minutes) spent in ED')
plt.ylabel('Density')
plt.legend()
plt.show()
```

From these results, it looks like the log-normal distribution is the best fit to our data. This is confirmed by a low value of the K-S test statistic (0.048) which indicates that data and fitted distribution are similar.

```{python}
plt.clf()

plt.hist(data, bins=30, color='skyblue', edgecolor='black', density=True, alpha=0.6)
x = np.linspace(min(data), max(data), 1000)
pdf_lognormal = stats.lognorm.pdf(x, *params_lognormal)
plt.plot(x, pdf_lognormal, 'g-', label="Log-Normal Fit")
plt.xlabel('Average time (in minutes) spent in ED')
plt.ylabel('Density')
plt.legend()
plt.show()
```

# Model fitting
Now that we've determine the distribution that best fits our outcome variable of interest, we can use an appropriate model, here a generalised linear model with log link, to try to understand what variables in our dataset are associated with ED wait times. 
* Note: the below code is an example of how you could fit a model; however, I ran out of time to do model-fitting properly.

```{python}
df_fit = df_ed_op.dropna(subset=['score'])

model = glm('score ~ state', data=df_fit, family=sm.families.Gaussian(link=sm.families.links.log())).fit()
# print(model.summary())
```